{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the SharpSpring Library Overview\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nUsage Examples\n\n\nSeveral examples have been recorded with prior versions of the Sharpspring_Common library as jupyter notebooks. To access, make sure you have the Anaconda python distribution installed, or at least jupyter-notebook.\n\n\njupyter-notebook\n - Opens the directory in the default browser.\n\n\nAccount Import\n\n\nCEF Donation Sum\n\n\nClear Sharpspring App\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    Sharpspring_Common_Update.md # Basic library documentation\nSharpspring_Common.py # Main library file",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-sharpspring-library-overview",
            "text": "For full documentation visit  mkdocs.org .",
            "title": "Welcome to the SharpSpring Library Overview"
        },
        {
            "location": "/#usage-examples",
            "text": "Several examples have been recorded with prior versions of the Sharpspring_Common library as jupyter notebooks. To access, make sure you have the Anaconda python distribution installed, or at least jupyter-notebook.  jupyter-notebook  - Opens the directory in the default browser.  Account Import  CEF Donation Sum  Clear Sharpspring App",
            "title": "Usage Examples"
        },
        {
            "location": "/#project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    Sharpspring_Common_Update.md # Basic library documentation\nSharpspring_Common.py # Main library file",
            "title": "Project layout"
        },
        {
            "location": "/Sharpspring_Common_Update/",
            "text": "Sharpspring_Common\n\n\nThis library contains standard functions used for interacting with the Sharpspring API.\n\n\nDependencies: requests, json, os, base64, datetime, csv, sys, time\n\n\nSend\n\n\nSend handles all actual requests to the Sharpspring API. Methods and parameters can be found at the \nSharpspring API Documentation\n. Note that all requests to the API use the POST method, regardless of the actual method signature used. In this example, the account_id and secret_key are set as global variables prior to being used here. This function attempts to handle the most common errors, although the response format seems to fluctuate sometimes, and the error catching itself is not guaranteed to be error free.\n\n\ndef send(m, p):\n\n    # Sets session ID using urandom for enhanced security\n    request = urandom(24)\n    requestID = b64encode(request).decode('utf-8')\n\n    data = {\n        'method': m,\n        'params': p,\n        'id': requestID\n    }\n\n\n    url = \"http://api.sharpspring.com/pubapi/v1/?accountID={}&secretKey={}\"\\\n          .format(account_id, secret_key)\n\n    dataj = json.dumps(data)\n\n    r = requests.post(\n        url,\n        data=dataj\n    )\n    try:\n        response = r.json()\n    except Exception:\n        print(r)\n        raise Exception('JSON Error')\n\n    if response['error']:\n        try:\n            if response['error'][0]['code'] == 301:\n                print(\"Duplicate hit\")\n                return response\n            else:\n                print('API Level error: ')\n                print(response)\n                raise Exception('System Error')\n        except KeyError:\n            print(response)\n    return response\n\n\n\nBatch Sending\n\n\nBatch sending is required for any uploads larger than 500 records. Server errors with processing large requests are common, and will generate a \nJSON Error\n response code. This seems to happen irrespective of request size, or time between requests. Both of these variables can be modified using the batch_size and sleep_length respectively.\n\n\n    def batch_send_objects(m, data):\n        batch_size = 450\n        sleep_length = 3\n        results = list()\n        batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n        for batch in batches:\n            objects = {'objects': batch}\n            temp = send(m, objects)\n            results.append(temp)\n            time.sleep(sleep_length)\n        return results\n\n\n\nDe-Duping\n\n\nThe de-dupe functions (including contact, accounts, opportunities, and opleads) each use the same basic structure, with different fields used for duplicate checking.\n\n\ndef de_dupe_contacts(contacts):\n    existing_contacts = getContacts()\n    old_emails = []\n    output = []\n    for old in existing_contacts:\n        old_emails.append(old['emailAddress'])\n    for contact in contacts:\n        if contact['emailAddress'] in old_emails:\n            pass\n        else:\n            output.append(contact)\n    return output\n\n\n\nImporting from CSV\n\n\nThe prep_from_file and related prep_ops functions are slightly overcomplicated CSV imports, modified to allow easy manipulation of fields on import. It is recommended to modify the field names in the csv itself before import to match the system names for simplicity.\n\n\ndef prep_from_file(filename):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        output = []\n        for row in reader:\n            temp = {}\n            for x in row:\n                temp[x] = row[x]\n            output.append(temp)\n    return output\n\n\ndef prep_ops(filename):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        output = []\n        for row in reader:\n            temp = {}\n            for x in row:\n                temp[x] = row[x]\n            temp['closeDate'] = datetime.datetime.strptime(temp['closeDate'], '%m/%d/%Y')\n            temp['closeDate'] = datetime.datetime.strftime(temp['closeDate'], '%Y-%m-%d')\n            output.append(temp)\n    return output\n\n\n\nIf you want to do a simple csv import without modifying fields, I recommend using a pandas Dataframe with the from_csv method, as I find this to be quicker and give you more flexibility in the result. To return the records to the standard list of dicts format for uploading, use the following:\n\n\nDataFrame.to_dict(orient='records')\n\n\n\nUpload and Update\n\n\nAll upload and update functions follow the same format seen below, with minor modifications for each type. Note that they all use the de-dupe functionality prior to sending. Input is required as a list of dictionaries.\n\n\ndef upload_opportunities(Opps):\n    filtered = de_dupe_ops(Opps)\n    if len(filtered) > 100:\n        return(batch_send_objects('createOpportunities', filtered))\n    else:\n        data = {'objects': filtered}\n        return(send('createOpportunities', data))\n\n\n\nGet\n\n\nGet functions again use a standard format for retrieving all records of a certain type, identified in the function name. Note that these functions are susceptible to variations in response format, such as the response being encapsulated in a list. improving the send methods can control these variations.\n\n\ndef getContacts():\n    method = 'getLeads'\n    offset = 0\n    records = list()\n    while True:\n        params = {'where': {}, 'offset': offset}\n        result = send(method, params)\n        for lead in result['result']['lead']:\n            records.append(lead)\n        if len(result['result']['lead']) < 500:\n            break\n        elif offset > 10000:\n            break\n        offset += 500\n\n    return(records)\n\n\n\nLinking\n\n\nLinking is among the more complicated parts of large scale imports, especially when working off of non-standard IDs which make the built - in CRM import tool useless. The below is an example for different required links in the system, using custom fields as common identifiers. The required inputs are tables that have \nalready been imported\n and thus will have the correct system ids. These can be obtained using the Get functions described above.\n\n\ndef link_opleads(opportunities, contacts):\n    count = 0\n    opleads = []\n    for contact in contacts:\n        for op in opportunities:\n            if (contact[**custom id field**] ==\n                    op[**custom id field**]):\n                count += 1\n                temp = {'opportunityID': op['id'], 'leadID': contact['id']}\n                opleads.append(temp)\n    data_final = {'objects': opleads}\n    print('{} opleads generated'.format(count))\n    return(opleads)\n\n\ndef link_accounts(accounts, contacts):\n    count = 0\n    new_contacts = []\n    for contact in contacts:\n        for account in accounts:\n            if (contact[**'company' or custom id field**] ==\n                    account['accountName']):\n                count += 1\n                temp = {'accountID': account['id'], 'id': contact['id']}\n                new_contacts.append(temp)\n    data_final = {'objects': new_contacts}\n    print('{} accounts linked'.format(count))\n    return(data_final)\n\n\ndef link_account_ops(ops, accounts):\n    count = 0\n    error_count = 0\n    good_results = []\n    bad_results = []\n    for op in ops:\n        for account in accounts:\n            if account[**custom id field**] is None:\n                account[**custom id field**] = ''\n            elif op[**custom id field**] is None:\n                op[**custom id field**] = ''\n            if op[**custom id field**] == account[**custom id field**]:\n                op['accountID'] = account['id']\n                good_results.append(op)\n                count += 1\n        if op['accountID'] == '':\n            error_count += 1\n            bad_results.append(op)\n        elif op['accountID'] is None:\n            print('WARNING unlinked op: ' + op['accountID'])\n    print('{} opportunities linked to accounts, with {} errors'\n          .format(count, error_count))\n    return good_results, bad_results\n\n\n\nDelete\n\n\nObviously, use this with care. Input is a standard table of records formatted as a list of dicts, and a method provided by the SharpSpring API Documentation. \nThere are no additional controls to deleting data - anything sent using this method will be deleted permanently.\n\n\ndef delete_records(records, m):\n    to_kill = []\n    for x in records:\n        temp = {'id': x['id']}\n        to_kill.append(temp)\n    data = {'objects': to_kill}\n    return(send(m, data))",
            "title": "Sharpspring Common"
        },
        {
            "location": "/Sharpspring_Common_Update/#sharpspring_common",
            "text": "This library contains standard functions used for interacting with the Sharpspring API.  Dependencies: requests, json, os, base64, datetime, csv, sys, time",
            "title": "Sharpspring_Common"
        },
        {
            "location": "/Sharpspring_Common_Update/#send",
            "text": "Send handles all actual requests to the Sharpspring API. Methods and parameters can be found at the  Sharpspring API Documentation . Note that all requests to the API use the POST method, regardless of the actual method signature used. In this example, the account_id and secret_key are set as global variables prior to being used here. This function attempts to handle the most common errors, although the response format seems to fluctuate sometimes, and the error catching itself is not guaranteed to be error free.  def send(m, p):\n\n    # Sets session ID using urandom for enhanced security\n    request = urandom(24)\n    requestID = b64encode(request).decode('utf-8')\n\n    data = {\n        'method': m,\n        'params': p,\n        'id': requestID\n    }\n\n\n    url = \"http://api.sharpspring.com/pubapi/v1/?accountID={}&secretKey={}\"\\\n          .format(account_id, secret_key)\n\n    dataj = json.dumps(data)\n\n    r = requests.post(\n        url,\n        data=dataj\n    )\n    try:\n        response = r.json()\n    except Exception:\n        print(r)\n        raise Exception('JSON Error')\n\n    if response['error']:\n        try:\n            if response['error'][0]['code'] == 301:\n                print(\"Duplicate hit\")\n                return response\n            else:\n                print('API Level error: ')\n                print(response)\n                raise Exception('System Error')\n        except KeyError:\n            print(response)\n    return response",
            "title": "Send"
        },
        {
            "location": "/Sharpspring_Common_Update/#batch-sending",
            "text": "Batch sending is required for any uploads larger than 500 records. Server errors with processing large requests are common, and will generate a  JSON Error  response code. This seems to happen irrespective of request size, or time between requests. Both of these variables can be modified using the batch_size and sleep_length respectively.      def batch_send_objects(m, data):\n        batch_size = 450\n        sleep_length = 3\n        results = list()\n        batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n        for batch in batches:\n            objects = {'objects': batch}\n            temp = send(m, objects)\n            results.append(temp)\n            time.sleep(sleep_length)\n        return results",
            "title": "Batch Sending"
        },
        {
            "location": "/Sharpspring_Common_Update/#de-duping",
            "text": "The de-dupe functions (including contact, accounts, opportunities, and opleads) each use the same basic structure, with different fields used for duplicate checking.  def de_dupe_contacts(contacts):\n    existing_contacts = getContacts()\n    old_emails = []\n    output = []\n    for old in existing_contacts:\n        old_emails.append(old['emailAddress'])\n    for contact in contacts:\n        if contact['emailAddress'] in old_emails:\n            pass\n        else:\n            output.append(contact)\n    return output",
            "title": "De-Duping"
        },
        {
            "location": "/Sharpspring_Common_Update/#importing-from-csv",
            "text": "The prep_from_file and related prep_ops functions are slightly overcomplicated CSV imports, modified to allow easy manipulation of fields on import. It is recommended to modify the field names in the csv itself before import to match the system names for simplicity.  def prep_from_file(filename):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        output = []\n        for row in reader:\n            temp = {}\n            for x in row:\n                temp[x] = row[x]\n            output.append(temp)\n    return output\n\n\ndef prep_ops(filename):\n    with open(filename) as f:\n        reader = csv.DictReader(f)\n        output = []\n        for row in reader:\n            temp = {}\n            for x in row:\n                temp[x] = row[x]\n            temp['closeDate'] = datetime.datetime.strptime(temp['closeDate'], '%m/%d/%Y')\n            temp['closeDate'] = datetime.datetime.strftime(temp['closeDate'], '%Y-%m-%d')\n            output.append(temp)\n    return output  If you want to do a simple csv import without modifying fields, I recommend using a pandas Dataframe with the from_csv method, as I find this to be quicker and give you more flexibility in the result. To return the records to the standard list of dicts format for uploading, use the following:  DataFrame.to_dict(orient='records')",
            "title": "Importing from CSV"
        },
        {
            "location": "/Sharpspring_Common_Update/#upload-and-update",
            "text": "All upload and update functions follow the same format seen below, with minor modifications for each type. Note that they all use the de-dupe functionality prior to sending. Input is required as a list of dictionaries.  def upload_opportunities(Opps):\n    filtered = de_dupe_ops(Opps)\n    if len(filtered) > 100:\n        return(batch_send_objects('createOpportunities', filtered))\n    else:\n        data = {'objects': filtered}\n        return(send('createOpportunities', data))",
            "title": "Upload and Update"
        },
        {
            "location": "/Sharpspring_Common_Update/#get",
            "text": "Get functions again use a standard format for retrieving all records of a certain type, identified in the function name. Note that these functions are susceptible to variations in response format, such as the response being encapsulated in a list. improving the send methods can control these variations.  def getContacts():\n    method = 'getLeads'\n    offset = 0\n    records = list()\n    while True:\n        params = {'where': {}, 'offset': offset}\n        result = send(method, params)\n        for lead in result['result']['lead']:\n            records.append(lead)\n        if len(result['result']['lead']) < 500:\n            break\n        elif offset > 10000:\n            break\n        offset += 500\n\n    return(records)",
            "title": "Get"
        },
        {
            "location": "/Sharpspring_Common_Update/#linking",
            "text": "Linking is among the more complicated parts of large scale imports, especially when working off of non-standard IDs which make the built - in CRM import tool useless. The below is an example for different required links in the system, using custom fields as common identifiers. The required inputs are tables that have  already been imported  and thus will have the correct system ids. These can be obtained using the Get functions described above.  def link_opleads(opportunities, contacts):\n    count = 0\n    opleads = []\n    for contact in contacts:\n        for op in opportunities:\n            if (contact[**custom id field**] ==\n                    op[**custom id field**]):\n                count += 1\n                temp = {'opportunityID': op['id'], 'leadID': contact['id']}\n                opleads.append(temp)\n    data_final = {'objects': opleads}\n    print('{} opleads generated'.format(count))\n    return(opleads)\n\n\ndef link_accounts(accounts, contacts):\n    count = 0\n    new_contacts = []\n    for contact in contacts:\n        for account in accounts:\n            if (contact[**'company' or custom id field**] ==\n                    account['accountName']):\n                count += 1\n                temp = {'accountID': account['id'], 'id': contact['id']}\n                new_contacts.append(temp)\n    data_final = {'objects': new_contacts}\n    print('{} accounts linked'.format(count))\n    return(data_final)\n\n\ndef link_account_ops(ops, accounts):\n    count = 0\n    error_count = 0\n    good_results = []\n    bad_results = []\n    for op in ops:\n        for account in accounts:\n            if account[**custom id field**] is None:\n                account[**custom id field**] = ''\n            elif op[**custom id field**] is None:\n                op[**custom id field**] = ''\n            if op[**custom id field**] == account[**custom id field**]:\n                op['accountID'] = account['id']\n                good_results.append(op)\n                count += 1\n        if op['accountID'] == '':\n            error_count += 1\n            bad_results.append(op)\n        elif op['accountID'] is None:\n            print('WARNING unlinked op: ' + op['accountID'])\n    print('{} opportunities linked to accounts, with {} errors'\n          .format(count, error_count))\n    return good_results, bad_results",
            "title": "Linking"
        },
        {
            "location": "/Sharpspring_Common_Update/#delete",
            "text": "Obviously, use this with care. Input is a standard table of records formatted as a list of dicts, and a method provided by the SharpSpring API Documentation.  There are no additional controls to deleting data - anything sent using this method will be deleted permanently.  def delete_records(records, m):\n    to_kill = []\n    for x in records:\n        temp = {'id': x['id']}\n        to_kill.append(temp)\n    data = {'objects': to_kill}\n    return(send(m, data))",
            "title": "Delete"
        }
    ]
}